[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Remote Sensing Learning Diary",
    "section": "",
    "text": "Julia Pieza Learning Diary\nHello, I’m Julia and this is my CASA0025 Remote Sensing Learning Diary. I’m currently completing the MSc in Urban Spatial Science at UCL whilst also working as a Senior Research and Intelligence officer at Brent Council.",
    "crumbs": [
      "Julia Pieza Learning Diary"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  About me",
    "section": "",
    "text": "After graduating with a History & Politics degree I began working at Brent first completing the NGDP Graduate Scheme and then joining the Data & Insight team.\nMy journey with GIS started at Brent where as an analyst I got the opportunity to learn how to work with geospatial data. Since then I’ve enjoyed the ability to work data analysis projects that help frontline services operate and make a difference to the lives of local residents.\nAt the moment I’m exploring the uses satellite data has in local government, for example addressing challenges like climate change the risks that more extreme weather events pose to urban settings.\nI’m looking forward to sharing my learning throughout this module and being able to share my thoughts on new ways of utilising remotely sensed data to make a difference on a local level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About me</span>"
    ]
  },
  {
    "objectID": "week_1.html",
    "href": "week_1.html",
    "title": "2  Week 1: What is Remote Sensing?",
    "section": "",
    "text": "2.1 Summary\nIn week 1 we were introduced to the basic concepts of Remote Sensing which I’ll cover briefly below.\nTo start with some definitions:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: What is Remote Sensing?</span>"
    ]
  },
  {
    "objectID": "week_1.html#summary",
    "href": "week_1.html#summary",
    "title": "2  Week 1: What is Remote Sensing?",
    "section": "",
    "text": "Remote sensing is the collection of data about the Earth from the atmosphere with various technologies that utilise electromagnetic (EM) waves.\nElectromagnetic waves are sources of EM energy used in remote sensing can come from sources like the Sun, or sensors such as LiDAR sending out their own light from a laser and then measuring the the return time for those laser pulses to identify information about the surface of the Earth.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: What is Remote Sensing?</span>"
    ]
  },
  {
    "objectID": "week_1.html#electromagnetic-waves",
    "href": "week_1.html#electromagnetic-waves",
    "title": "2  Week 1: What is Remote Sensing?",
    "section": "2.2 Electromagnetic Waves",
    "text": "2.2 Electromagnetic Waves\nThe electromagnetic spectrum is essentially the energy that makes remote sensing work.\nAll matter that is above absolute zero temperatures (-273.15°C) radiates EM energy due to molecules moving against one another, and there are several complex processes which can change how much EM energy is radiated, such as temperature. Essentially all natural objects receive and re-emit energy, and we can measure and use this data to identify the material the light is reflecting off of such as vegetation.\n\n\n\nElectromagnetic Spectrum, Source: Wikipedia\n\n\nSource: Wikipedia\n\n2.2.1 Active and Passive Sensors\n\n\n\nSensors, Source: Wikipedia\n\n\nPassive sensing refers to using an external source of EM energy to operate. Commonly the EM energy emitted by the Sun is used to measure the reflection and absorption of light by objects and this data is collected by Satellites which produce images of the Earth.\nFor passive sensors it is important to remember that weather such as clouds or particles in the atmosphere can interfere with the EM energy and cause scattering which can interfere with the image produced. One example of this is Rayleigh scattering where nitrogen and oxygen particles in the air interact with the Sun’s emitted EM waves to create the sunset effect.\nWays in which this can be corrected will be expanded upon in Week 3.\nActive sensors rely on a source of EM to emit and receive electromagnetic waves and measure the backscatter of said wave. For example LiDAR emits rapid laser pulses to collect information about the object or surface based on the return time of the laser pulse.\nAs such active sensors such as Synthetic Aperture Radar (SAR) sends electromagnetic waves that are not on the light spectrum to the Earth which are not interfered with by clouds or atmospheric particles allowing the sensor to capture information at night.\nWe can also think about it in simpler terms, the cameras that exist on our mobile phones are light sensors, allowing us to take images of the environment around us without necessarily making physical contact with it.\n\n\n2.2.2 Resolutions\nThere are 4 resolutions in remote sensed imagery.\n\n\n\n\n\n\n\n\nResolution\nDefinition\nUse\n\n\n\n\nSpatial\nThe size of each pixel in an image corresponds to an area of the Earth’s surface, e.g. one pixel can correspond to as little as a few centimetres (high resolution) to 10km.\nImages of the Earth can be used to detect objects such as buildings. A high spatial resolution allows us to see more detail.\n\n\nSpectral\nSenors which can distinguish between more closely spaced waves are said to measure more Bands of imagery i.e. more wavelengths.\nA high spectral resolution allows us to identify particular types of minerals, vegetation or other features because each object has a unique spectral signature.\n\n\nTemporal\nThe time it takes for a satellite to make an orbit and end at the same observation area. Satellites can have orbits as short as 1 day, or much longer.\nAllows us to see change over time for a particular area of observation.\n\n\nRadiometric\nInformation in each pixel of the image, measured by the number of bits. A 4-bit image indicates there are 16 digital values (grayscale shades) in an image. The more bits the more subtle information can be discerned from the image.\nCan create really detailed images when being able to detect very slight differences in energy, can detect very subtle changes in environments or land use.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: What is Remote Sensing?</span>"
    ]
  },
  {
    "objectID": "week_1.html#spectral-signatures",
    "href": "week_1.html#spectral-signatures",
    "title": "2  Week 1: What is Remote Sensing?",
    "section": "2.3 Spectral signatures",
    "text": "2.3 Spectral signatures\nDifferent surfaces such as concrete or grassland will reflect EM waves in different ways, creating unique ‘signatures’ that help us identify the type of surface. These signatures reflect various characteristics such as differences between dry and moist ground.\nFor example vegetation tends to absorb visible light (blue and red light specifically used for photosynthesis) and reflects near-infrared energy which can be captured and summarised in a chart. The health of vegetation can be measured in this way as unhealthy plants with less chlorophyll will reflect differently to healthy plants.\n\n\n\nSpectral signatures, Source:NASA",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: What is Remote Sensing?</span>"
    ]
  },
  {
    "objectID": "week_1.html#applications",
    "href": "week_1.html#applications",
    "title": "2  Week 1: What is Remote Sensing?",
    "section": "2.4 Applications",
    "text": "2.4 Applications\n\n2.4.1 Open Source Data - Sentinel and Landsat\nThis week we started working with some open satellite data from the Sentinel and Landsat satellites. Sentinel satellites are run by the Copernicus Programme at the European Space Agency and provide high spatial resolution imagery, 10m, 20, and 60m per pixel resolution depending on the Band used.\nThe Landsat program started by NASA and the US Geological Survey in 1972 has provided imagery across the world. The latest operational Landsat satellites are Landsat 8 and 9 providing 15m, 30m and 100m resolution depending on the Band used.\n\n\n2.4.2 SNAP Case study: Gdansk\nDuring the practical I used SNAP to interact with Sentinel satellite imagery for the Polish city of Gdansk, learning how to render these in true colour (RGB) and extract statistics from the data such as the nature of vegetation, e.g. whether the soil is wet or dry.\n\n\n\nGdansk, Source:Sentinel2b\n\n\n\n\n2.4.3 Scatter plot\nI started the practical by looking at recent satellite imagery of Gdansk in January 2026 where large snowfall was reflected in the scatter plot I produced. In order to compute this using the near-infrared and green light that plant matter reflects I had to understand the RGB bands used to produce the scatter plot from B4 - B8. In the image for Gdansk and the surrounding areas we can see a lot of wet bare soil which makes sense. \n\n\n\nGdansk SNAP scatter plot, Source: Sentinel2b\n\n\nThe scatter plot above shows the Band 4 (x axis) and Band 8 (y axis), which are the red and near-infrared bands, so here the lower values of Band 8 and Band 4 are indicating wet bare soil which makes sense given the winter period.\n\n\n2.4.4 Tasseled Cap Transformation\nThe Tasseled Cap Transformation is a method used to analyse the spectral data from multiple bands into a standardised image which incorporates information about surface brightness, vegetation greenness and moisture content.\nTo start this process I managed to clip the city boundaries for my raster image importing a shapefile into SNAP and masking it.\n\n\n\nGdansk Clipped SNAP, Source: Sentinel2b\n\n\nI struggled with following the practical and computing the brightness, greenness and wetness bands in SNAP but managed to produce an image attempting to represent the Tasseled Cap function which is a part of Principal Component analysis. Perhaps due to the presence of snow the result is being distorted somehow, but more likely that I computed this image wrong.\nBrightness is associated with bare soil or manmade surfaces helping to identify urban areas. Greeness is associated with green vegetation and wetness with moisture. The green pictured below is correctly identifying the large forest area in the northwest of the clipped boundary. In general there is a lot of vegetation across the city and we cannot see any water bodies as the sea has been clipped out.\nWhat is displayed in pink usually shows man-made surface like concrete and generally reflects built up areas. This accurately reflects the airport in the west of the map, and areas of the city centre, but I would have expected to see more of this around the city centre and it is picking up some bare soil fields in the eastern part of the map.\nI was also expecting the outer fields to show up more brightly given the presence of snow as visible in the image above.\n\n\n\nGdansk TCT, Source: Sentinel2b\n\n\nTo check this further I computed a false composite image using Bands B8, B4 and B3 to check how the near-infrared reflecting plant matter shows up. This does largely match the PCA tasseled cap transformation above with the greenness and the near-infrared reflection captured in red matching well. However I assume due to the presence of snow around the fields in the eastern part of the map is mostly bare soil which is why it appears pink in the PCA and white in the false composite image.\n\n\n\nGdansk False Colour, Source: Sentinel2b\n\n\nUnfortunately I was unable to compute the selected points of interest using SNAP as it simply took too long to export into a GeoTiff file. I did not manage to complete the R portion of this practical but I hope that in future weeks I can manage to do this.\nDespite this, the initial experimentation with SNAP allowed me to start to understand how multispectral sensors the Bands they use can be manipulated to produce interesting images to get a better interpretation of the vegetation and urban areas across the city.\nUltimately, I think using data from January wasn’t a great choice given the impact of snow but it definitely made me realise just how much goes into selecting appropriate satellite imagery given that atmospheric events like clouds, fog, snow can alter the usefulness of the images.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: What is Remote Sensing?</span>"
    ]
  },
  {
    "objectID": "week_1.html#reflections",
    "href": "week_1.html#reflections",
    "title": "2  Week 1: What is Remote Sensing?",
    "section": "2.5 Reflections",
    "text": "2.5 Reflections\nI spent some time reading ahead for the upcoming weeks and was really interested to discover that both multispectral and SAR imagery can be combined to extract information about the natural and urban environments across the world. I found the visual explanation of how this can be done from (Schulte to Bühne and Pettorelli 2018) to be a good introduction and I enjoyed reading about the various examples of environmental monitoring. One of these was outlined by (Ban 2003) exploring how to classify crops such as wheat and corn under good and bad growth conditions to demonstrate how combining Landsat data with SAR gave them the most accurate classification of crop health. I can imagine how valuable this data is during unprecedented weather events that can exacerbate food shortages in vulnerable regions.\nPersonally, working at a Local Council I would be interested to read more about the urban applications of these techniques for monitoring smaller areas of vegetation such as urban farms, or looking at the impact to infrastructure under extreme weather events. Combined with studies of how environmental changes impact citizens and their health outcomes would provide an insightful avenue for using satellite data alongside geodemographic data.\n\n\n\n\nBan, Yifang. 2003. “Synergy of Multitemporal ERS-1 SAR and Landsat TM Data for Classification of Agricultural Crops.” Canadian Journal of Remote Sensing 29 (4): 518–26. https://doi.org/10.5589/m03-014.\n\n\nSchulte to Bühne, Henrike, and Nathalie Pettorelli. 2018. “Better Together: Integrating and Fusing Multispectral and Radar Satellite Imagery to Inform Biodiversity Monitoring, Ecological Research and Conservation Science.” Methods in Ecology and Evolution 9 (4): 849–65. https://doi.org/10.1111/2041-210X.12942.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: What is Remote Sensing?</span>"
    ]
  },
  {
    "objectID": "week_3.html",
    "href": "week_3.html",
    "title": "3  Week 3: Corrections",
    "section": "",
    "text": "3.1 Summary\nIn Week 3 we covered the corrections that are often necessary to make remotely sensed data usable for analysis.\nAlthough most remotely sensed data nowadays comes ‘analysis ready’ it was interesting and useful to understand these concepts in more detail.\nIn this chapter I’ll summarise:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3: Corrections</span>"
    ]
  },
  {
    "objectID": "week_3.html#summary",
    "href": "week_3.html#summary",
    "title": "3  Week 3: Corrections",
    "section": "",
    "text": "Radiative transfer and atmospheric correction\nGeometric distortions and corrections\nOrthorectification/topographic correction\nRadiometric calibration\n\n\n3.1.1 Radiative transfer and atmospheric correction\nGoing back to concepts from Week 1 radiative transfer was helpful to recap in the context of understanding why atmospheric correction is needed for remotely sensed data.\nI enjoyed the video from Karen Joyce explaining this concept in an accessible way using the example of emitted sunlight (Top of Atmosphere irradiance) which is reflected off of a surface and into a sensor (also referred to as At Sensor Radiance). However, some light from the sun will end up being scattered in the atmosphere (diffuse irradiance) and will also get recorded by the sensor.\nWhereas, ground scanners measure total radiance i.e. all light reflected off the sun, sensors in the atmosphere or in space will measure At Sensor Radiance. To get an accurate image of the ground the atmospheric component (additive path radiance) needs to be subtracted from the At Sensor Radiance i.e. the sensor produced image.\n\n\n\nAtmospheric correction, Source: ESA\n\n\nThere are several ways to correct for this haze-like effect, this is especially necessary when we want to extract accurate information about the biophysical nature of the image e.g. plant health or temperature as these will get distorted by the light interactions with particles in the atmosphere. The list below is not an exhaustive list of methods but a flavour of what’s available and how it works.\n\n\n\n\n\n\n\n\nCorrection\nMethod\nPros & Cons\n\n\n\n\nDark Object Substraction\nFind the darkest value from the image collected and subtract this value from each pixel. Works on the principle that some dark objects e.g. water bodies will have near zero reflectance, and these values can then be subtracted from all the pixels in the image to remove atmospheric haze.\nPros: relative normalisation to a reference image often not expensive\nCons: there may not be a dark object within an image to normalise to\n\n\nPseudo Invariant Features\nTakes features which do not change in an image e.g. a road or carpark, this will have a temporally stable spectral reflectance and can be used to normalise the imagery using linear regression to remove the atmospheric effects.\nPros: inexpensive if a reference image has a PIF\nCons: if there is no PIF feature it can be unreliable, assumes reference image atmospheric distortion will be similar across time\n\n\nFLAASH (Fast line of sight atmospheric analysis)\nRemoves atmospheric effects by modelling the atmosphere using a technique called MODTRAN which simulates the atmospheric path and enables the estimation of light scattering between the ground and sensor.\nIt requires more detailed information about the sensor and is a more expensive tool to use, however the calculations based on physics enable precise correction.\nPros: an absolute atmospheric correction is usually more precise\nCons: requires more detailed atmospheric data and often expensive models to model atmospheric condition at the time of the image taken\n\n\n\n\n\n3.1.2 Geometric distortion and corrections: \nApart from light there are other ways in which the way the sensor collects an image can result in some need for correction, these are geometric distortions such as:\n\nView angle - whether the sensor collects data straight down or at a Nadir or at an angle\nTopography - hills vs flatlands\nRotation of the earth - from satellite data, depending on the path of flight of the satellite\nOr things like wind if data is collected from a plane\n\nThe most common way to correct for geometric distortions is by having Ground Control Points (GCPs) to match existing features from a reference picture. Examples of this are:\n\nLocal maps \nGPS data from another device \nOr another satellite image\n\nHere we look for points that wouldn’t have changed, potentially natural features like rivers although these can change so sometimes manmade features can work better.\n\n\n3.1.3 Orthorectification /topographic correction\nGeorectification refers to adding coordinates to an image and orthorectification refers to removing the distortions to make an image at a nadir angle i.e. straight down. \nThis is a subset of geometric correction.\nIn order to make an orthorectified image the elevation needs to be known, as well as GCPs which then allow a correction algorithm to transform the raw data into an orthorectified image. There are various algorithms to do this often depending on the sensor, image resolution and the desired level of accuracy.\n\n\n3.1.4 Radiometric calibration\nThis is the process of converting the image brightness captured as Digital Numbers by sensors into spectral radiance.\nDigital Numbers refer to the brightness captured in a remotely sensed image, but they have no units.\nSpectral radiance is the amount of light within a band from a sensor, measured in Watts, per metre squared, per steradian (angle of view), per nanometre (wavelenght).\nThis is necessary when collecting images across time, which will be influenced by atmospheric and geometric distortions so to compare these more accurately we need to calibrate the pixel values using information about the sensor (e.g. how it was calibrated).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3: Corrections</span>"
    ]
  },
  {
    "objectID": "week_3.html#applications",
    "href": "week_3.html#applications",
    "title": "3  Week 3: Corrections",
    "section": "3.2 Applications",
    "text": "3.2 Applications\n\n3.2.1 Image enhancements: NVDI\nUsing a technique called ratioing I was able to calculate the Normalised Difference Vegetation Index which works by comparing near-infrared bands to red light wavelengths to reflect healthy and less healthy vegetation.\n\n\n\nNVDI, Source: GeoPard Agriculture\n\n\nI was able to calculate NVDI by using Landsat 8 images for Gdansk using the following formula and subtracting the near-infrared band (Band 5) from red light wavelenght (Band 4), and then dividing by the sum of both.\n\n\n\nGdansk NVDI, Source: Landsat 8\n\n\nThe image above reflects vegetation in August 2025, where we can see dense vegetation across the forest area of Lasy Oliwskie. Nevertheless it is actually more difficult to assess the health of vegetation without more comparison over time as the lower NVDI figures reflect water and built up areas of sparse vegetation, hence for future analysis it might make more sense to subset this area and compare the NVDI for more dense vegetation areas across time to look at plant health. As this imagery is from August 2025 there may be areas where plant health is worse due to heat which last year was between 25-31 degrees Celsius.\n\n\n3.2.2 Filtering\nI was also able to filter the image to show everything above NVDI 0.2, this can be used to mask out vegetation from built up areas which can be useful for further monitoring.\n As before there are some caveats with using this to accurately assert the health of vegetation as more monitoring over time would be needed.\n\n\n3.2.3 Texture analysis\nNext I wanted to look at potentially extracting more of the built up areas of the city using the GLCMTextures library to calculate the more ‘jagged’ texture of man-made materials like concrete by comparing variance to the adjacent pixels.\nI played around with the settings and looked at the GLCM_entropy metric to look at low entropy areas such as calm water or smooth surfaces and higher entropy for areas like urban settings.\n\n\n\nGdansk GLCM_Textures, Source: Landsat 8\n\n\n\n\n3.2.4 Data fusion - PCA\nData fusion was something I was really interested in after reading Schulte to Bühne and Pettorelli (2018) and here it is applied in a slightly simpler sense, by taking the texture produced above as a new Band effectively appending it to our original raster dataset. Using this we can better see the differences between built up and vegetation across Gdansk, although as mentioned in the lecture this where remote sensing becomes more of an art, as there is a lot of room to transform the data in many different ways.\nFollowing on from Week 1, PCA is used to reduce the dimensionality of remotely sensed data, in this week’s practical I combined the texture data with the spectral reflectance Landsat 8 data for Gdansk.\nThis was done to reduce the number of variables in my data helping to enhance my image whilst keeping the key patterns of the dataset e.g. my vegetation vs urban areas divide.\n\n\n\nGdansk PCA Source: Landsat 8\n\n\nIn the image above PCA1 captures most of the variance between the vegetation and more built up areas and can be used by a machine learning algorithm to predict trends and patterns visible in the data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3: Corrections</span>"
    ]
  },
  {
    "objectID": "week_3.html#reflections",
    "href": "week_3.html#reflections",
    "title": "3  Week 3: Corrections",
    "section": "3.3 Reflections",
    "text": "3.3 Reflections\nI think the analysis I was able to produce could be better to show more distinction between the built-up areas and vegetation but for a first go with texture and raster PCA I am glad it generally worked.\nAs I was looking at various research which utilised satellite imagery around the Gdansk area I found an article (Tysiac, Dąbal, and Widerski 2026) which used multispectral imagery and LiDAR technology for heritage mapping purposes. As an ex-history graduate this grabbed my interest and I found that high resolution imagery (1m), historical maps and machine learning classification to perform analysis in Google Earth Engine to attempt to digitally locate the remnants of a fortification from the 17th century called Gdanska Glowa.\n\n\n\n\nSchulte to Bühne, Henrike, and Nathalie Pettorelli. 2018. “Better Together: Integrating and Fusing Multispectral and Radar Satellite Imagery to Inform Biodiversity Monitoring, Ecological Research and Conservation Science.” Methods in Ecology and Evolution 9 (4): 849–65. https://doi.org/10.1111/2041-210X.12942.\n\n\nTysiac, Pawel, Joanna Dąbal, and Tadeusz Widerski. 2026. “Multispectral Data and LiDAR for Enhanced Georeferencing of Gdanska Glowa’s 17th-Century Fortifications.” Archaeometry: Bulletin of the Research Laboratory for Archaeology and the History of Art, Oxford University 68 (1): 94–106. https://doi.org/10.1111/arcm.70032.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3: Corrections</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ban, Yifang. 2003. “Synergy of Multitemporal ERS-1\nSAR and Landsat TM Data for Classification of\nAgricultural Crops.” Canadian Journal of Remote Sensing\n29 (4): 518–26. https://doi.org/10.5589/m03-014.\n\n\nSchulte to Bühne, Henrike, and Nathalie Pettorelli. 2018. “Better\nTogether: Integrating and Fusing Multispectral and Radar\nSatellite Imagery to Inform Biodiversity Monitoring, Ecological Research\nand Conservation Science.” Methods in Ecology and\nEvolution 9 (4): 849–65. https://doi.org/10.1111/2041-210X.12942.\n\n\nTysiac, Pawel, Joanna Dąbal, and Tadeusz Widerski. 2026.\n“Multispectral Data and LiDAR for Enhanced\nGeoreferencing of Gdanska Glowa’s 17th-Century Fortifications.”\nArchaeometry: Bulletin of the Research Laboratory for Archaeology\nand the History of Art, Oxford University 68 (1): 94–106. https://doi.org/10.1111/arcm.70032.",
    "crumbs": [
      "References"
    ]
  }
]